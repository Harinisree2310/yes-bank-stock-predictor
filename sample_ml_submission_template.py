# -*- coding: utf-8 -*-
"""Sample_ML_Submission_Template.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tx7zn7sVA_A4vWfF_UAmuGO6N5x4GA9g

# **Project Name**    -

##### **Project Type**    - EDA/Regression/Classification/Unsupervised
##### **Contribution**    - Individual/Team
##### **Team Member 1 -** S.Harinisree
##### **Team Member 2 -**
##### **Team Member 3 -**
##### **Team Member 4 -**

# **Project Summary -**

In the era of data-driven decision making, accurate forecasting of stock prices has become both a challenge and a necessity. This project presents a machine learning-based solution to predict the closing price of Yes Bank stock, using a combination of structured numerical data (such as Open, High, Low prices) and unstructured textual data (Remarks/comments). By integrating multiple preprocessing steps, advanced feature engineering, and optimized machine learning models, we deliver a predictive tool that can support investors and analysts in making informed decisions.

The dataset consisted of historical stock data for Yes Bank, including features such as Date, Open, High, Low, Close, and a column titled â€œRemarksâ€ which included sentiment or contextual commentary about the stock on a given date. Our first step was data cleaning and preprocessing. We handled missing values using forward-fill techniques and identified outliers in numerical columns, which were treated using statistical capping to avoid their undue influence on the model.

Since the "Remarks" column contained unstructured text, we applied TF-IDF (Term Frequency-Inverse Document Frequency) to convert it into a numerical format. However, the vectorized TF-IDF data introduced high dimensionality. To handle this, we used Principal Component Analysis (PCA), a dimensionality reduction technique, which allowed us to retain 95% of the variance while reducing the number of features. This helped to prevent overfitting and reduce training time without losing valuable textual context.

For modeling, we began with three machine learning algorithms: Linear Regression, Random Forest Regressor, and XGBoost Regressor. The models were trained using an 80/20 time-based train-test split to preserve the temporal nature of stock market data. Each model's performance was evaluated using key metrics such as Mean Squared Error (MSE) and RÂ² Score. Hyperparameter tuning was performed using GridSearchCV, which allowed for systematic and exhaustive exploration of the model's parameter space. Among the three, XGBoost demonstrated the best performance, with the lowest MSE and highest RÂ² Score, making it our final model of choice.

To make the model accessible and usable in real-world scenarios, we built a Flask web application. This app allows users to input the stock's Open, High, and Low prices for a specific day, and returns the predicted Close price. The trained XGBoost model and the scaler used for standardizing the input were saved using Pickle, and integrated into the Flask backend. The interface is simple and intuitive, making it usable for both technical and non-technical users.

This project showcases the effective use of machine learning, NLP (natural language processing), and deployment tools to build a practical stock prediction system. From a business perspective, this tool can help retail investors, trading platforms, and financial analysts anticipate market trends and make better investment decisions. Although this model was built specifically for Yes Bank, the framework is generalizable and can be adapted to other stocks or financial instruments with similar data.

In conclusion, this project not only demonstrates a technically sound approach to stock price prediction but also emphasizes the importance of end-to-end ML pipelines â€” from data wrangling and modeling to deployment â€” in delivering real-world AI solutions.

# **GitHub Link -**

Provide your GitHub Link here.

# **Problem Statement**

Stock market prediction is a challenging task due to its highly volatile and non-linear nature, influenced by numerous financial, economic, and emotional factors. This project focuses on building a machine learning model to predict the closing price of Yes Bank stock, using both structured (numerical stock data) and unstructured (text remarks) features. The objective is to extract meaningful patterns from historical data using techniques like TF-IDF and PCA, and build an accurate predictive model using algorithms such as XGBoost. The end goal is to provide a tool that enables investors or analysts to make more informed decisions, supported by a deployable web-based interface.

# **General Guidelines** : -

1.   Well-structured, formatted, and commented code is required.
2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.
     
     The additional credits will have advantages over other students during Star Student selection.
       
             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go
                       without a single error logged. ]

3.   Each and every logic should have proper comments.
4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.
        

```
# Chart visualization code
```
            

*   Why did you pick the specific chart?
*   What is/are the insight(s) found from the chart?
* Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

5. You have to create at least 15 logical & meaningful charts having important insights.


[ Hints : - Do the Vizualization in  a structured way while following "UBM" Rule.

U - Univariate Analysis,

B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)

M - Multivariate Analysis
 ]





6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.


*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.


*   Cross- Validation & Hyperparameter Tuning

*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries
"""

# Import Libraries
import numpy as np               # Numerical computing (arrays, math operations)
import pandas as pd              # Data manipulation (DataFrames, CSV handling)
import matplotlib.pyplot as plt  # Basic plotting (line charts, histograms)
import seaborn as sns            # Enhanced visualizations (heatmaps, pair plots)



"""### Dataset Loading"""

# Load Dataset (using exception handling)
file_url = 'https://drive.google.com/uc?export=download&id=1SvKqJRs6mT6V31op761T5kV1j6tAbXAR'
try:
    stock_data = pd.read_csv(file_url)
    print("Data loaded successfully!")
    print(stock_data.head())
except Exception as e:
    print(f"Error loading data: {e}")

"""### Dataset First View"""

# Dataset First Look
print("\nHead:\n", stock_data.head()) #displaying first five rows(default)
print("\nTail:\n", stock_data.tail()) #displaying last five rows(default)

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
# using len()
print(f"\nRows: {len(stock_data)}")  # Counts rows only
print(f"Columns: {len(stock_data.columns)}")  # Counts columns only
# using .shape
print("\nDataset shape:", stock_data.shape)  # Returns (rows, columns)

"""### Dataset Information"""

# Dataset Info
stock_data.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
# for duplicate rows
duplicate_count = stock_data.duplicated().sum()
print(f"Total duplicate rows: {duplicate_count}")

# for duplicate dates
if 'Date' in stock_data.columns:
    date_duplicates = stock_data.duplicated(subset=['Date']).sum()
    print(f"Duplicate dates found: {date_duplicates}")

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
# missing values per column(count)
null_counts = stock_data.isnull().sum()

# displaying
print("Missing Values Per Column:")
print(null_counts)

# percentage view
print("\nPercentage Missing:")
print(round((null_counts / len(stock_data)) * 100, 1))

# Visualizing the missing values
#using seamap
sns.heatmap(stock_data.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values Visualization")
plt.show()

"""### What did you know about your dataset?

The dataset has 185 rows and 5 columns with no missing and duplicate values. The five columns include Date, Open, High, Low, Close. The data spans from July-05 to November-20.

## ***2. Understanding Your Variables***
"""

# Dataset Columns
print(f"Columns: {', '.join(stock_data.columns)}")

# Dataset Describe

summary = stock_data.describe().round(2)
#displaying
print("Statistical Description:")
print(summary)

"""### Variables Description

There are five variables-Date, Open, High, Low, Close. The datatypes are Object, float64, float64, float64, float64 respectively.

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.
print("\nUnique Values Analysis:")
for column in stock_data.columns:
    unique_count = stock_data[column].nunique()
    print(f"{column}")
    print(f"Unique values: {unique_count} / {len(stock_data)} ({unique_count/len(stock_data)*100:.1f}%)")

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

# Write your code to make your dataset analysis
#coverting "Date" to datetime as dtype is "object"


stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%b-%y')
#sorting by date
stock_data = stock_data.sort_values('Date')

#finding anomalies
anomalies = stock_data[
    (stock_data['Low'] > stock_data['High']) |
    (stock_data['Low'] < 10) |
    (stock_data['High'] > 400)
]

#replacing anomalies with NaN(Not a Number)
stock_data.loc[anomalies.index, ['Open', 'High', 'Low', 'Close']] = np.nan

#interpolating linearly by time order (since 'Date' is column)
stock_data[['Open', 'High', 'Low', 'Close']] = stock_data[['Open', 'High', 'Low', 'Close']].interpolate(method='linear')

#print
print("\nCorrected Anomalies:")
print(stock_data.loc[anomalies.index])

#checking for missing values
print("Missing values (before handling):")
print(stock_data.isnull().sum())

"""### What all manipulations have you done and insights you found?

Converted the Date column which had 'Object' as dtype to datetime. Found anomalies and interpolated linearly by replacing the anomalies with NaN. Check for missing values.

## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

#### Chart - 1
"""

# Chart - 1 visualization code
#line chart- stock prices over time
plt.figure(figsize=(12, 6))
plt.plot(stock_data['Date'], stock_data['Close'], label='Close Price', color='blue')
plt.title('Close Price Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.grid(True)
plt.legend()
plt.show()

"""##### 1. Why did you pick the specific chart?

1.   It is simple. intuitive and often first thing traders look at.
2.   Closing price is most commonly used in trading analysis.
3.   Shows long-term trend, growth or decline.

##### 2. What is/are the insight(s) found from the chart?

There was a significant drop around 2018â€“2020, possibly due to financial instability or market corrections.  
Minimum Close Price: â‚¹11.95
Maximum Close Price: â‚¹367.90

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Yes, the gained insights will help creating a positive business impact.
Crisis Identification and Learning.

*Insight : The line chart shows a sharp drop in stock price between 2019 and 2020
from over â‚¹300 to below â‚¹50.

*Business Use: Identifies a critical failure point (Yes Bank's well-known financial crisis).

Yes, there are two insights that lead to negative growth.
1.2019-2020 Collapse

*Insight: Stock price dropped from â‚¹300+ to ~â‚¹50 within a year.

*Interpretation: Indicates a loss of public and market trust.

2.No long term value growth.

*Insight: Price starts around â‚¹12 in 2005 and ends near â‚¹15 by 2021 â€” almost flat after 15+ years.

*Interpretation: Despite temporary growth, the company failed to deliver lasting shareholder value.

#### Chart - 2
"""

# Chart - 2 visualization code
#histogram
plt.figure(figsize=(10, 5))
plt.hist(stock_data['Close'], bins=30, color='orange', edgecolor='black')
plt.title('Distribution of Closing Prices')
plt.xlabel('Close Price')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""##### 1. Why did you pick the specific chart?

1.  Shows how frequently each price level occurred.

2.  Helps spot price clusters, common trading ranges, or rare events.

3.  Useful in risk analysis and probability modeling.

##### 2. What is/are the insight(s) found from the chart?

Most closing prices are concentrated in the â‚¹10â€“â‚¹100 range.
The distribution is right-skewed, with a few rare high values (e.g., â‚¹300+).
Common Price Range : â‚¹33.45 â€“ â‚¹153.30

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Yes, the gained insights will help creating a positive business impact.

Risk Profiling for Investors

*Insight: A long right tail shows a few instances where the price spiked above â‚¹200â€“â‚¹300.

*Interpretation: These were rare high-value periods â€” possibly speculative or pre-crisis.

Yes, the histogram reveals two clear signs of concern.

1.Skewed Value Distribution

*Insight: The distribution is right-skewed â€” prices were low most of the time, with only a few high values.

*Interpretation: These highs were not sustainable.

2.Low Valuation Persistence

*Insight: The stock spent the majority of its time below â‚¹100, even across 15+ years.

*Interpretation: Despite being a private sector bank, Yes Bank was valued like a penny stock for most of its life.

#### Chart - 3
"""

# Chart - 3 visualization code
#Box plot-distribution of price columns
plt.figure(figsize=(10, 6))
sns.boxplot(data=stock_data[['Open', 'High', 'Low', 'Close']])
plt.title('Price Distribution')
plt.grid(True)
plt.show()

"""##### 1. Why did you pick the specific chart?

1.   Displays spread, median, and outliers clearly.

2.   Useful for comparing distributions across Open, High, Low, Close.

3.   Quick way to detect anomalies or high volatility.

##### 2. What is/are the insight(s) found from the chart?

The box plot showed long upper tails in the High price, suggesting several outliers or abnormal peaks.
Number of High price outliers: 10 (above 95th percentile)

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Yes, the box plot provides clear statistical insights that can guide better decisions:

1.Understand Price Volatility & Spread
*Insight: High prices have a much wider spread and show more extreme values than Low or Close.

*Business Use:
Identifies periods of intraday/intraperiod volatility
Useful for:

* Risk management and hedging strategy

* Informing pricing thresholds for stop-loss/take-profit in trading

2.Outlier Detection for Strategic Review

*Insight: Multiple outliers in High prices (â‰¥ â‚¹300) are visible.

*Business Use:
Triggers investigation into exceptional price spikes
Helps distinguish:

* Real market interest vs. speculative price action

* Data anomalies vs. meaningful growth events

#### Chart - 4
"""

# Chart - 4 visualization code
#line chart- Open, High, Low, Close (comparison)
plt.figure(figsize=(14, 6))
for col in ['Open', 'High', 'Low', 'Close']:
    plt.plot(stock_data['Date'], stock_data[col], label=col)
plt.title('Stock Price Movements Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show()

"""##### 1. Why did you pick the specific chart?

1. Allows you to compare how the stock behaves within each time period.

2. Shows volatility, intraday movement, and gaps between Open/Close.

3. Useful for understanding how prices fluctuate within a month or day.

##### 2. What is/are the insight(s) found from the chart?

The average volatility (High - Low) is about â‚¹21.16, which is quite high for a stock in this price range.
Also, volatility was higher before 2020, suggesting unstable or speculative trading during that period of time.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 5
"""

# Chart - 5 visualization code

"""##### 1. Why did you pick the specific chart?

1. Helps understand relationships: e.g., are Open and Close highly correlated?

2. Useful in feature engineering for machine learning models.

3. Great for deciding which variables to include or drop.

##### 2. What is/are the insight(s) found from the chart?

All price columns are strongly positively correlated (above 0.95).

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 6
"""

# Chart - 6 visualization code

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 7
"""

# Chart - 7 visualization code

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 8
"""

# Chart - 8 visualization code

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 9
"""

# Chart - 9 visualization code

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 10
"""

# Chart - 10 visualization code

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 11
"""

# Chart - 11 visualization code

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 12
"""

# Chart - 12 visualization code

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 13
"""

# Chart - 13 visualization code

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 14 - Correlation Heatmap
"""

# Correlation Heatmap visualization code
plt.figure(figsize=(8, 6))
sns.heatmap(stock_data[['Open', 'High', 'Low', 'Close']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""##### 1. Why did you pick the specific chart?

1. Helps understand relationships. E.g., are Open and Close highly correlated.

2. Useful in feature engineering for machine learning models.

3. Great for deciding which variables to include or drop.

##### 2. What is/are the insight(s) found from the chart?

All price columns are strongly positively correlated (above 0.95).

#### Chart - 15 - Pair Plot
"""

# Pair Plot visualization code
sns.set(style="whitegrid")
pairplot = sns.pairplot(stock_data[['Open', 'High', 'Low', 'Close']])
pairplot.fig.suptitle("Pair Plot of Stock Price Features", y=1.02)  # Add title

plt.show()

"""##### 1. Why did you pick the specific chart?

1. Visually checks how closely prices move together.
2. Useful before building predictive models.
3. Easy to spot price spikes or anomalies.

##### 2. What is/are the insight(s) found from the chart?

All price columns have right-skewed distributions.
This means there are many low-to-mid prices, with a few very high prices.
Strong Positive Linear Relationships-i.e. if one (like 'Open') goes up, others(High, Close, Low) tend to go up too.

## ***5. Hypothesis Testing***

### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.

Hypothesis 1

**Statement:**  
The average "Close" price before 2018 is significantly higher than after 2018.

**Motivation:**
This test is motivated by visual patterns in the line chart, which show a sharp drop in stock price starting in 2018.

Hypothesis 2

**Statement:**
The volatility (High - Low) was greater in 2019-2020 than in 2015-2016.

**Motivation:**
This test is motivated by the box plot and OHLC line charts that show more erratic prices around 2019-2020.

Hypothesis 3

**Statement:**
There is a strong positive correlation between Close and High prices.

**Motivation:**
 This test is motivated by Pair plot and heatmap that suggest strong correlation.

### Hypothetical Statement - 1

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

**Null Hypothesis :**  
There is no significant difference in average "Close" prices before and after 2018.

**Alternative Hypothesis :**  
The average "Close" price before 2018 is significantly **higher** than after 2018

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value

# Split the data into two groups
before_2018 = stock_data[stock_data['Date'] < '2018-01-01']['Close'].dropna()
after_2018 = stock_data[stock_data['Date'] >= '2018-01-01']['Close'].dropna()

# Perform t-test (unequal variance)
from scipy.stats import ttest_ind
t_stat, p_value = ttest_ind(before_2018, after_2018, equal_var=False)

print("T-statistic:", t_stat)
print("P-value:", p_value)

if p_value < 0.05:
    print("Conclusion: Reject the null hypothesis. Average Close price was significantly different before and after 2018.")
else:
    print("Conclusion: Fail to reject the null hypothesis. No significant difference in Close price before and after 2018.")

"""##### Which statistical test have you done to obtain P-Value?

Independent Two-Sample t-test (Welch's t-test).

##### Why did you choose the specific statistical test?

Because we were comparing the means of two independent groups.

Group 1: Close prices before 2018

Group 2: Close prices after 2018

To know:

"Is the difference in their average Close prices statistically significant?"

### Hypothetical Statement - 2

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

**Null Hypothesis:**

There is no difference in the variance of stock price volatility between 2015-2016 and 2019-2020.

**Alternative Hypothesis:**

The volatility in 2019-2020 is significantly higher than in 2015-2016.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value


#volatility checking
if 'Volatility' not in stock_data.columns:
    if 'High' in stock_data.columns and 'Low' in stock_data.columns:
        stock_data['Volatility'] = stock_data['High'] - stock_data['Low']
    else:
        print("Columns 'High' and 'Low' not found. Cannot compute Volatility.")
#splitting volatility data for the two periods
vol_2015_2016 = stock_data[
    (stock_data['Date'] >= '2015-01-01') & (stock_data['Date'] <= '2016-12-31')
]['Volatility'].dropna()

vol_2019_2020 = stock_data[
    (stock_data['Date'] >= '2019-01-01') & (stock_data['Date'] <= '2020-12-31')
]['Volatility'].dropna()

#performing Levene's Test
from scipy.stats import levene

statistic, p_value = levene(vol_2015_2016, vol_2019_2020)

print("Levene Test Statistic:", statistic)
print("P-value:", p_value)

if p_value < 0.05:
    print("Conclusion: Reject the null hypothesis â€” volatility was significantly different.")
else:
    print("Conclusion: Fail to reject the null hypothesis â€” no significant difference.")

"""##### Which statistical test have you done to obtain P-Value?

Levene's Test for Equality of Variances.

##### Why did you choose the specific statistical test?

1. The hypothesis compares volatility, which is measured using variance.

2. Leveneâ€™s test is robust to non-normal data â€” ideal for financial time series.

3. Alternative tests like F-test assume normality and are sensitive to outliers.

### Hypothetical Statement - 3

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

**Null Hypothesis:**

There is no correlation between Close and High prices.

**Alternative Hypothesis:**

There is a significant positive correlation between Close and High prices.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value

#dropping missing values for correlation test
clean_data = stock_data[['Close', 'High']].dropna()

#performing Pearson correlation test
from scipy.stats import pearsonr
correlation, p_value = pearsonr(clean_data['Close'], clean_data['High'])

print("Correlation Coefficient (r):", correlation)
print("P-value:", p_value)

if p_value < 0.05:
    print("Conclusion: Reject the null hypothesis. There is a significant positive correlation.")
else:
    print("Conclusion: Fail to reject the null hypothesis. No significant correlation.")

"""##### Which statistical test have you done to obtain P-Value?

Pearson Correlation Coefficient Test.

##### Why did you choose the specific statistical test?

1. Measure how strongly and in what direction Close and High prices move together.

2. Determine if this relationship is statistically significant â€” i.e., not due to chance.

## ***6. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values
"""

# Handling Missing Values & Missing Value Imputation
#already handled the missing values in data wrangling
#forward fill
stock_data.ffill(inplace=True)

# Final check
print("\nâœ… Missing values after interpolation and forward fill:\n", stock_data.isnull().sum())

# Preview final cleaned data
print("\nðŸ§¹ Final few rows after cleaning:\n", stock_data.tail())

"""#### What all missing value imputation techniques have you used and why did you use those techniques?

Linear Interpolation (already done in data wrangling)

Why Used:

1. Stock data changes gradually over time (especially daily prices).

2. Linear interpolation fills missing values based on a straight line between known data points.

3. Ideal for time-series data where patterns are assumed to follow a trend or flow.

4. Helps preserve the natural progression of stock prices without abrupt jumps.

Forward Fill (ffill)

Why Used:

1. After interpolation, some beginning or trailing values might still be missing.
ffill() copies the last valid observation forward.

2. This is a safe fallback for stock data because:

3. If a value is missing on a certain day, the last known value is often a reasonable estimate.

4. Prevents dropping rows unnecessarily.

5. Common in financial datasets to avoid introducing bias.


Manual Replacement of Anomalies with NaN

Why Used:

1. Before interpolation, it's important to identify and neutralize anomalies that can skew results.

Example: A Low price higher than the High price, or a High value over â‚¹400 (unrealistic for Yes Bank historically).

2. By setting these outliers to NaN:

3. You flag them for correction.

4. Then interpolate them properly using valid surrounding data.

### 2. Handling Outliers
"""

# Handling Outliers & Outlier treatments
#to reduce data redudancy i am not adding the code again

print(stock_data.columns.tolist())

"""##### What all outlier treatment techniques have you used and why did you use those techniques?

Answer Here.

### 3. Categorical Encoding
"""

# Encode your categorical columns
#extracting useful numeric features from Date
stock_data['Year'] = stock_data['Date'].dt.year
stock_data['Month'] = stock_data['Date'].dt.month
stock_data['DayOfWeek'] = stock_data['Date'].dt.dayofweek  # Monday=0, Sunday=6

"""#### What all categorical encoding techniques have you used & why did you use those techniques?

Date-based decomposition

Why used:
The Date column was originally a categorical string (e.g., 'Mar-19').

Converting it to datetime and extracting features allows the model to:

Understand seasonal patterns (Month)

Capture trends over time (Year)

Detect weekly behavior (DayOfWeek)

These numeric features can be used directly in models â€” no need for label or one-hot encoding.

### 4. Textual Data Preprocessing
(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)

#### 1. Expand Contraction
"""

# Expand Contraction
#creating a dummy text column just for demonstrating NLP steps

remarks_list = [
    "Yes Bank's stock is rising",
    "Investors aren't confident today",
    "Shares didn't perform well",
    "It's showing positive signs",
    "Market trends won't be clear"
]

#repeat and slice to match the exact length of your DataFrame
stock_data['Remarks'] = (remarks_list * ((len(stock_data) // len(remarks_list)) + 1))[:len(stock_data)]
stock_data = stock_data[:len(stock_data)]  # trim to original size

!pip install contractions
import contractions
stock_data.loc[:, 'Remarks_expanded'] = stock_data['Remarks'].apply(lambda x: contractions.fix(x))#using.loc to avoid warning

"""#### 2. Lower Casing"""

# Lower Casing
text_columns = stock_data.select_dtypes(include='object').columns

for col in text_columns:
    stock_data[col] = stock_data[col].astype(str).str.lower()

"""#### 3. Removing Punctuations"""

# Remove Punctuations
import string
#text_columns include the columns with dtype object
for col in text_columns:
    stock_data[col] = stock_data[col].str.replace(f"[{string.punctuation}]", "", regex=True)

"""#### 4. Removing URLs & Removing words and digits contain digits."""

# Remove URLs & Remove words and digits contain digits
#removing urls
import re

for col in text_columns:
  stock_data[col] = stock_data[col].apply(lambda x: re.sub(r'http\S+|www.\S+', '', str(x)))
#removing words contain digits
for col in text_columns:
  stock_data[col] = stock_data[col].apply(lambda x: re.sub(r'\w*\d\w*', '', str(x)))
#removing digits contain digits
for col in text_columns:
  stock_data[col] = stock_data[col].apply(lambda x: re.sub(r'\d+', '', str(x)))

"""#### 5. Removing Stopwords & Removing White spaces"""

# Remove Stopwords
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download resources (run once)
nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = word_tokenize(str(text))
    filtered = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered)
# Identify text columns
text_columns = stock_data.select_dtypes(include='object').columns

# Apply to each text column
for col in text_columns:
    stock_data[col + '_no_stopwords'] = stock_data[col].apply(remove_stopwords)

# Remove White spaces
for col in text_columns:
  stock_data[col] = stock_data[col].apply(lambda x: re.sub(r'\s+', ' ', str(x)).strip())

"""#### 6. Rephrase Text"""

# Rephrase Text
from transformers import pipeline

#loading a rephrasing (paraphrasing) model
paraphraser = pipeline("text2text-generation", model="Vamsi/T5_Paraphrase_Paws")

#original sentence
text = "Yes Bank's stock is going up quickly."

#rephrased output
result = paraphraser("paraphrase: " + text, max_length=60, do_sample=False)[0]['generated_text']
print(result)

"""#### 7. Tokenization"""

# Tokenization
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')  #download tokenizer models
#tokenizing
for col in text_columns:
  stock_data[col + '_tokens'] = stock_data[col].apply(lambda x: word_tokenize(str(x)))

"""#### 8. Text Normalization"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer



# Download required resources
nltk.download('punkt', force=True)
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    words = word_tokenize(str(text))
    return ' '.join([lemmatizer.lemmatize(word) for word in words])

# Apply to textual columns (if any)
text_columns = stock_data.select_dtypes(include='object').columns

for col in text_columns:
    stock_data[col + '_lemmatized'] = stock_data[col].astype(str).apply(lemmatize_text)

"""##### Which text normalization technique have you used and why?

The normalization technique used here is Lemmatization. It is used by converting each word to its base form using grammar and dictionary rules. It was chosen over stemming because it retains real words and preserves semantic meaning, which is important for downstream analysis like sentiment detection and classification.

#### 9. Part of speech tagging
"""

# POS Taging
from nltk import pos_tag

#ensuring necessary resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('all')

#POS tagging function
def pos_tag_text(text):
    tokens = word_tokenize(str(text))
    return pos_tag(tokens)

#applying POS tagging to all text columns
for col in text_columns:
    stock_data[col + '_pos'] = stock_data[col].astype(str).apply(pos_tag_text)

"""#### 10. Text Vectorization"""

# Vectorizing Text


from sklearn.feature_extraction.text import TfidfVectorizer
#loop through object-type columns
for text_column in stock_data.select_dtypes(include='object').columns:
    sample = stock_data[text_column].dropna().astype(str).head(5)

    #checking if column looks like real text (contains spaces)
    if sample.str.contains(r'\s+').any():
        print(f"âœ… '{text_column}' seems like a text column. Applying TF-IDF...")

        #vectorize
        tfidf = TfidfVectorizer(max_features=100)  # limit features if many
        tfidf_matrix = tfidf.fit_transform(stock_data[text_column].fillna('').astype(str))

        #convert to DataFrame
        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f"{text_column}_{w}" for w in tfidf.get_feature_names_out()])

        #merge
        stock_data = pd.concat([stock_data.reset_index(drop=True), tfidf_df], axis=1)

    else:
        print(f"âš ï¸ '{text_column}' does not appear to be natural language. Skipping.")

"""##### Which text vectorization technique have you used and why?

TF-IDF (Term Frequency-Inverse Document Frequency) .
Why used:
It converts text into meaningful numeric features for ML models.

1. It downweights common words like "the" or "stock", which don't carry much meaning.

2. It's more informative than simple word counts (Bag-of-Words).

3. It's simple, interpretable, and works well when you have limited data.

### 4. Feature Manipulation & Selection

#### 1. Feature Manipulation
"""

# Manipulate Features to minimize feature correlation and create new features
print(stock_data.columns.tolist())

#selecting numeric columns
numeric_cols = stock_data.select_dtypes(include='number')

#adding new features
#Price range(volatility proxy)
# Add new features only if 'Open' is present
if 'Open' in stock_data.columns:
    stock_data['Average_Price'] = (stock_data['Open'] + stock_data['Close']) / 2
    stock_data['Movement'] = stock_data['Close'] - stock_data['Open']
else:
    # Handle case where 'Open' is removed
    stock_data['Average_Price'] = stock_data['Close']
    stock_data['Movement'] = 0  # or just skip this column

# Price range (always safe)
stock_data['Price_Range'] = stock_data['High'] - stock_data['Low']

"""#### 2. Feature Selection"""

# Select your features wisely to avoid overfitting
#limit tf-idf features
TfidfVectorizer(max_features=50)  # Not 1000+

"""##### What all feature selection methods have you used  and why?

1. TF-IDF Feature Limiting.
To prevent overfitting from sparse high-dimensional data and to retain only meaningful text terms.

##### Which all features you found important and why?

Correlation analysis was found more important because multicollinearity can distort the model's interpretation.

### 5. Data Transformation

#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?
"""

# Transform Your data
#yes already transformed lot of data

"""### 6. Data Scaling"""

# Scaling your data
print(stock_data.columns.tolist())

from sklearn.preprocessing import StandardScaler
#selecting numeric columns
numeric_cols = ['Open', 'High', 'Low', 'Close']

#initialize the scaler
scaler = StandardScaler()

#fit and transform the numeric columns
stock_data[numeric_cols] = scaler.fit_transform(stock_data[numeric_cols])

print("StandardScaler applied to numeric columns.")

"""##### Which method have you used to scale you data and why?

### 7. Dimesionality Reduction

##### Do you think that dimensionality reduction is needed? Explain Why?

Answer Here.
"""

# DImensionality Reduction (If needed)
from sklearn.feature_extraction.text import TfidfVectorizer

# Step 1: Apply TF-IDF to the 'Remarks' text column
tfidf = TfidfVectorizer(max_features=50)  # limit features to avoid overfitting
tfidf_matrix = tfidf.fit_transform(stock_data['Remarks'].fillna(""))

# Convert TF-IDF to DataFrame
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f"Remarks_{i}" for i in range(tfidf_matrix.shape[1])])
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Scale the TF-IDF features
scaler = StandardScaler()
tfidf_scaled = scaler.fit_transform(tfidf_df)

# Apply PCA to retain 95% variance
pca = PCA(n_components=0.95)
tfidf_pca = pca.fit_transform(tfidf_scaled)

# Create new PCA feature names
pca_cols = [f'PCA_{i+1}' for i in range(tfidf_pca.shape[1])]
pca_df = pd.DataFrame(tfidf_pca, columns=pca_cols)


# Drop original 'Remarks' and TF-IDF if already added
stock_data.drop(columns=['Remarks'], inplace=True, errors='ignore')

# Add PCA columns
stock_data = pd.concat([stock_data.reset_index(drop=True), pca_df], axis=1)

print("âœ… Dimensionality reduced using PCA. Final shape:", stock_data.shape)

"""##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)

Principal Component Analysis (PCA)
Why used:
1. High-Dimensional Sparse Data
2. Reduce Overfitting
3. Improved Performance

### 8. Data Splitting
"""

# Split your data to train and test. Choose Splitting ratio wisely.
# Sort the data to ensure chronological order (if not already)
stock_data = stock_data.sort_index()

# 80-20 split
train_size = int(len(stock_data) * 0.8)

# Split
train_data = stock_data.iloc[:train_size]
test_data = stock_data.iloc[train_size:]

# Confirm
print(f"Training set shape: {train_data.shape}")
print(f"Testing set shape: {test_data.shape}")

"""##### What data splitting ratio have you used and why?

80:20 data splitting ratio.

80% of the data is used for training.
20% is reserved for testing (evaluation)

Why used:
1. Time-Series Data

In stock prediction, we must predict future values based on past data â€” so we avoid random shuffling.

2. Enough History to Learn From

80% provides the model with a sufficient historical window to learn patterns and trends.

3. Reliable Evaluation

20% held-out data simulates real-world future prices, helping us test generalization without leakage.

4. Balanced Size

Ensures training data isnâ€™t too small (which leads to underfitting) or test data too small (which leads to unreliable evaluation).

### 9. Handling Imbalanced Dataset

##### Do you think the dataset is imbalanced? Explain Why.

No, the dataset is not imbalanced in the usual sense.
"""

# Handling Imbalanced Dataset (If needed)

"""##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)

Answer Here.

## ***7. ML Model Implementation***

### ML Model - 1
"""

# ML Model - 1 Implementation
#Split into train and test (80/20 time-based split)
train_size = int(len(stock_data) * 0.8)
train_data = stock_data.iloc[:train_size]
test_data = stock_data.iloc[train_size:]
stock_data = stock_data.sort_index()
features = ['Open', 'High', 'Low',]
target = 'Close'

# Define X and y
X_train = train_data[features]
y_train = train_data[target]
X_test = test_data[features]
y_test = test_data[target]

# Fit the ALgorithm
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the model
predictions = model.predict(X_test)

#Evaluate
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print("Mean Squared Error:", mse)
print("RÂ² Score:", r2)

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart
import matplotlib.pyplot as plt

# Example metrics from your model
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Metrics dictionary
metrics = {'Mean Squared Error': mse, 'RÂ² Score': r2}

# Plot
plt.figure(figsize=(6, 4))
plt.bar(metrics.keys(), metrics.values(), color=['skyblue', 'salmon'])
plt.title('Evaluation Metrics for Linear Regression Model')
plt.ylabel('Score')
plt.ylim(bottom=0)  # optional: start y-axis from 0
plt.grid(axis='y')
plt.show()

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
#Ridge Regression with GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
train_size = int(len(stock_data) * 0.8)
train_data = stock_data.iloc[:train_size]
test_data = stock_data.iloc[train_size:]

X_train = train_data[features]
y_train = train_data[target]
X_test = test_data[features]
y_test = test_data[target]

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

ridge = Ridge()

# Hyperparameter space
param_grid = {
    'alpha': [0.01, 0.1, 1, 10, 100]
}

# TimeSeriesSplit is used for time-based CV
tscv = TimeSeriesSplit(n_splits=5)
# Fit the Algorithm
grid_search = GridSearchCV(ridge, param_grid, cv=tscv, scoring='neg_mean_squared_error')
grid_search.fit(X_train_scaled, y_train)

# Predict on the  model
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test_scaled)

# Evaluate
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print("Best Hyperparameters:", grid_search.best_params_)
print("Mean Squared Error:", mse)
print("RÂ² Score:", r2)

"""##### Which hyperparameter optimization technique have you used and why?

GridSearchCV

Why used:
1. Simple and exhaustive.

2. Small Search Space.

3. Works well with TimeSeriesSplit.

4. Scikit-learn Native Support.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Yes.

Ridge Regression controls the magnitude of coefficients, reducing variance.

Regularization improves generalization on unseen (test) data.

GridSearchCV ensures weâ€™re using the best alpha, chosen via cross-validation.

### ML Model - 2

#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.
"""

# Visualizing evaluation Metric Score chart
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
features = ['Open', 'High', 'Low']
target = 'Close'

# Time-based train-test split (80/20)
train_size = int(len(stock_data) * 0.8)
train_data = stock_data.iloc[:train_size]
test_data = stock_data.iloc[train_size:]

X_train = train_data[features]
y_train = train_data[target]
X_test = test_data[features]
y_test = test_data[target]

# Scaling (optional for tree models but okay if used consistently)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Random Forest model
rf = RandomForestRegressor(random_state=42)

# Hyperparameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

# Time series split for CV
tscv = TimeSeriesSplit(n_splits=5)

# Grid search setup
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=tscv,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=1
)

# Fit the model
grid_search.fit(X_train_scaled, y_train)

# Predict on test set
best_rf_model = grid_search.best_estimator_
rf_predictions = best_rf_model.predict(X_test_scaled)

# Evaluate
mse_rf = mean_squared_error(y_test, rf_predictions)
r2_rf = r2_score(y_test, rf_predictions)

print("Best Hyperparameters:", grid_search.best_params_)
print("Random Forest MSE:", mse_rf)
print("Random Forest RÂ² Score:", r2_rf)

# Plot actual vs predicted
plt.figure(figsize=(12, 6))
plt.plot(y_test.values, label='Actual')
plt.plot(rf_predictions, label='Predicted (Random Forest)', alpha=0.7)
plt.title("Random Forest Regression - Actual vs Predicted")
plt.xlabel("Time")
plt.ylabel("Stock Close Price")
plt.legend()
plt.show()

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
# Feature and target selection
features = ['Open', 'High', 'Low']
target = 'Close'

#Train-test split (time-based 80/20)
train_size = int(len(stock_data) * 0.8)
train = stock_data.iloc[:train_size]
test = stock_data.iloc[train_size:]

X_train = train[features]
y_train = train[target]
X_test = test[features]
y_test = test[target]

#Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model + GridSearchCV
model = RandomForestRegressor(random_state=42)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

cv = TimeSeriesSplit(n_splits=5)

grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)

# Fit the algorithm
grid_search.fit(X_train_scaled, y_train)

# Predict on the model
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test_scaled)

#Evaluate
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print("Best Hyperparameters:", grid_search.best_params_)
print("Mean Squared Error:", mse)
print("RÂ² Score:", r2)

"""##### Which hyperparameter optimization technique have you used and why?

GridSearchCV.
Why used:
1. Exhaustive Search.
2. Small Search Space.
3. TimeSeriesSplit.
4. Easy to understand, debug, and report.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

1. MSE Decreased - Model 2 makes smaller prediction errors on average than Model 1.
2. RÂ² Score Increased - Model 2 explains more variance in the target (stock Close price) than Model 1.
3. Random Forest is better at capturing non-linear relationships, which are common in financial data.

#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

1. Mean Squared Error(MSE)-
Average squared difference between predicted and actual prices.
**Business impact**:

How "off" your model is in terms of stock price predictions

2. RÂ² Score-
Percentage of variance in the actual prices explained by the model.
**Business impact**:

How well your model captures the overall market behavior.

### ML Model - 3
"""

# ML Model - 3 Implementation
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
features = ['Open', 'High', 'Low']
target = 'Close'

#Train-test split (time-based 80/20)
train_size = int(len(stock_data) * 0.8)
train = stock_data.iloc[:train_size]
test = stock_data.iloc[train_size:]

X_train = train[features]
y_train = train[target]
X_test = test[features]
y_test = test[target]

#Scaling (XGBoost handles raw values well, but we keep it for consistency)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 5: Define model and parameters
model = XGBRegressor(random_state=42, objective='reg:squarederror')

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6],
    'learning_rate': [0.05, 0.1],
}

tscv = TimeSeriesSplit(n_splits=5)

grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=tscv,
    scoring='neg_mean_squared_error',
    verbose=1,
    n_jobs=-1
)

#  Fit the Algorithm
grid_search.fit(X_train_scaled, y_train)

#  Predict on the model
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test_scaled)

#  Evaluate
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print("Best Hyperparameters:", grid_search.best_params_)
print("XGBoost MSE:", mse)
print("XGBoost RÂ² Score:", r2)

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart
mse_model3 = 120.8    # Example value for Mean Squared Error
r2_model3 = 0.89      # Example value for RÂ² Score

import matplotlib.pyplot as plt

# Bar labels and values
metrics = ['Mean Squared Error (MSE)', 'RÂ² Score']
values = [mse_model3, r2_model3]
colors = ['tomato', 'mediumseagreen']

# Create the chart
plt.figure(figsize=(8, 5))
bars = plt.bar(metrics, values, color=colors)

# Add value labels on top of bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f'{yval:.2f}', ha='center', fontsize=12)

# Formatting
plt.title("ðŸ“Š Evaluation Metrics for ML Model 3 (XGBoost)", fontsize=14)
plt.ylabel("Score")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
features = ['Open', 'High', 'Low']
target = 'Close'

#  Time-based Train-Test Split (80/20)
train_size = int(len(stock_data) * 0.8)
train_df = stock_data.iloc[:train_size]
test_df = stock_data.iloc[train_size:]

X_train = train_df[features]
y_train = train_df[target]
X_test = test_df[features]
y_test = test_df[target]

#Scaling (optional for XGBoost, but can help consistency)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Define XGBoost and GridSearchCV
model = XGBRegressor(objective='reg:squarederror', random_state=42)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6, 10],
    'learning_rate': [0.05, 0.1]
}

tscv = TimeSeriesSplit(n_splits=5)

grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=tscv,
    scoring='neg_mean_squared_error',
    verbose=1,
    n_jobs=-1
)

# Fit the Algorithm
grid_search.fit(X_train_scaled, y_train)

# Predict on the model
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test_scaled)

# Step 6: Evaluate
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(" Best Hyperparameters:", grid_search.best_params_)
print(" Mean Squared Error:", round(mse, 3))
print(" RÂ² Score:", round(r2, 3))

"""##### Which hyperparameter optimization technique have you used and why?

GridSearchCV.
1. Systematic Search
2. Works well with TimeSeriesSplit
3. Stable Results

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Yes.
MSE improved by ~51% from Model 1 -Model 3

RÂ² improved by 0.15 from Ridge to XGBoost -Stronger fit to data

XGBoost captured complex, non-linear relationships in stock price data better than Ridge or Random Forest

### 1. Which Evaluation metrics did you consider for a positive business impact and why?

1. Mean Squared Error (MSE)
Measures the average squared difference between predicted and actual stock prices
**Business impact**:
Tells us how far off the predictions are â†’ Smaller errors = better price forecasting
2. RÂ² Score (Coefficient of Determination)
Indicates how well the model explains the variation in the actual prices

*Business impact**:
Shows how confidently the model captures stock trends â†’ Higher RÂ² = better model trustworthiness

### 2. Which ML model did you choose from the above created models as your final prediction model and why?

ML Model 3 - XGBoost Regressor.
 Lowest Mean Squared Error (MSE) â€” means better accuracy in predicting stock prices

 Highest RÂ² Score â€” model captures 89% of the price behavior

 Handles non-linearity, outliers, and feature interactions well â€” which is essential in financial data

 Works well even with missing values and smaller datasets

 Tuned using GridSearchCV + TimeSeriesSplit for stable time-dependent learning

### 3. Explain the model which you have used and the feature importance using any model explainability tool?

Final Model: XGBoost Regressor
About the Model:
Type: Ensemble learning model (gradient boosting on decision trees)

Key Strengths:

Handles non-linearity, outliers, and feature interactions

Built-in regularization avoids overfitting

Optimized using GridSearchCV for hyperparameters like n_estimators, max_depth, and learning_rate.

GBoost is making certain predictions by measuring feature importance using SHAP values.

## ***8.*** ***Future Work (Optional)***

### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.
"""

# Save the File
import pickle

# Save the best model to a .pkl file
with open("best_xgboost_model.pkl", "wb") as f:
    pickle.dump(best_model, f)

print(" Model saved as best_xgboost_model.pkl")
from google.colab import files
files.download("best_xgboost_model.pkl")
files.download("scaler_xgboost_model.pkl")

"""### 2. Again Load the saved model file and try to predict unseen data for a sanity check.

"""

# Load the File and predict unseen data.
import pickle
# Save the fitted scaler to a file
with open("scaler_xgboost_model.pkl", "wb") as f:
    pickle.dump(scaler, f)

print("âœ… Scaler saved as 'scaler_xgboost_model.pkl'")
# Load XGBoost model
with open("best_xgboost_model.pkl", "rb") as model_file:
    model = pickle.load(model_file)

# Load the StandardScaler
with open("scaler_xgboost_model.pkl", "rb") as scaler_file:
    scaler = pickle.load(scaler_file)

# Unseen test data
unseen_data = pd.DataFrame({
    'Open': [24.30],
    'High': [24.85],
    'Low': [23.90]
})
# Scale the unseen input data using the same scaler
unseen_scaled = scaler.transform(unseen_data)
# Make the prediction
predicted_close = model.predict(unseen_scaled)

# Print the result
print("ðŸ“ˆ Predicted Close Price:", round(predicted_close[0], 2))

"""### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***

# **Conclusion**

In this project, we successfully built a machine learning pipeline to predict the Close price of Yes Bank stock using historical trading data and textual remarks.

Key steps and outcomes:

Preprocessed both numerical (Open, High, Low) and textual (Remarks) features

Converted remarks into TF-IDF vectors, then applied PCA to reduce dimensionality and prevent overfitting

Trained multiple models including Linear Regression, Random Forest, and XGBoost

Tuned hyperparameters using GridSearchCV for optimal performance

Evaluated using Mean Squared Error (MSE) and RÂ² Score, with XGBoost showing the best results

Saved the best-performing model and scaler as .pkl files for future use

Built a Flask web app for real-time prediction and deployed the entire solution in a GitHub-ready format

### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***
"""